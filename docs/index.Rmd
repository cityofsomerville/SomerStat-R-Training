---
title: "Filter observations"
output: 


learnr::tutorial:
progressive: true
allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(Lahman)
library(RSocrata)
library(stringr)

socrataUrl <-"https://data.somervillema.gov/resource" ## not sensitive

generateUrl <- function (datasetID) paste(
  socrataUrl, 
  paste(datasetID, 'json', sep="."), 
  sep="/"
)

## all of this is actually precomputed so the people can pick up anywhere in the excercise
datasetID <- "pfjr-vzaw"
url <- generateUrl(datasetID)   
happinessSurveyData <- RSocrata::read.socrata(url)


tutorial_options(exercise.eval = FALSE,
exercise.timelimit = 60,
# A checker function that compares the expected and actual results
exercise.checker = function(label, user_code, solution_code, check_code, envir_result, evaluate_result, ...) {
  if(user_code == solution_code){
    return(list(correct = TRUE, message = "That's exactly how we did it!"))
  }
  user_result <- eval(parse(text = user_code), envir = envir_result)
  correct_result <- eval(parse(text = solution_code), envir = envir_result)
  if (nrow(user_result) == nrow(correct_result) && all.equal(user_result, correct_result) == TRUE) {
    return(list(correct = TRUE, message = "Great job!"))
  } else {
    return(list(correct = FALSE, message = "Check your answer and try again."))
  }
}
)


knitr::opts_chunk$set(error = TRUE)
```

## Welcome to our R (pun intended) rundown!

To merge two quotes from Sam's Rolodex of sayings I've heard in the past week or so, "R is the second best tool to do 5 million things and there are 5 thousand ways of doing each of these things." This tutorial is not meant to be a resource that you have open as you start your first project, it is meant to allow you to use the three best tools of R progress (Google, Google, and Google) to effectively write code that looks similar to how the other members of SomerStat write code in R. In an attempt to have you see the value of this similar coding style, I will show you the code that I (Derek) wrote in my first major project and the functionally identical code that Sam would have told me to use if this training existed when I started. This is by no means law, and if you prefer something by all means tell us about it and there's a good chance we will agree and change how we do things! 

Derek's code to combine two datasets on a column (i.e. there is an id field shared between two files and I want to merge these columns):

```{r, eval = FALSE}
merged <- merge(x = merged, y = geolocationData, by = 'id', all.x = TRUE)
```

Sam's much more readable code:

```{r, eval = FALSE}
merged <- merged %>% dplyr::left_join(geolocationData, by = 'id')
```

For those natural coders among us, you may be about to google what I already have, and yes, under the hood left_join takes a very convoluted path to doing nearly exactly this base R line with lots of error checking. Dplyr abstracts a lot of common lines of code (like merging two datasets) away and makes the code much more readable, and is widely used throughout the office. Let's load in some data and try some other features of dplyr out! In case of getting stuck, all of the answers are in an R file that probably lives right next to the one you ran to open the browser on your localhost unless I miraculously fixed the web-hosting problem. We do not care / track if you ended up getting the little textbox to be green, I just thought it would make the tutorial a bit more fun and interactive.

The data we will be using comes from our annual Happiness Survey (insert description of this here), which this website reads from the [Open Data Portal](https://data.somervillema.gov/Health-Wellbeing/Somerville-Happiness-Survey-Responses-2011-2021/pfjr-vzaw/about_data)

There is a good chance that your time here will include analyzing data in R and/or reading/writing data to/from the Open Data Portal, so I hope this little tutorial is more helpful than harmful.

### Common mistakes

Me (Derek) thinking I can code in R

## Loading in the Happiness Survey Data from the Open Data Portal

This is roughly the workflow that I use to read data from the Open Data portal.

First I make sure to load in all the packages, I use pacman to do this nasty install vs load in business for me. 
```{r filterex1, exercise = TRUE}
if (!require(pacman)){
  install.packages("pacman")
}
pacman::p_load(RSocrata, tidyverse) ## this loads in RSocrata and the entire tidyverse, which includes dplyr and many of the packages we use!
```

```{r filterex2, exercise = TRUE}

socrataUrl <-"https://data.somervillema.gov/resource" ## not sensitive

generateUrl <- function (datasetID) paste(
  socrataUrl, 
  paste(datasetID, 'json', sep="."), 
  sep="/"
)

```

```{r filterex3, exercise = TRUE}
## if the bottom of this exercise says 404 not found, you do not have the right datasetID
datasetID <- 'some-string' ## TODO
url <- generateUrl(datasetID)   
happinessSurveyData <- RSocrata::read.socrata(url)

```

```{r filterex3-hint-1}
# Check for the dataset id here, I typically find it in the url! <https://data.somervillema.gov/Health-Wellbeing/Somerville-Happiness-Survey-Responses/pfjr-vzaw/about_data>
# click Next Hint to see the solution!
```


```{r filterex3-solution}
## TODO, find the datasetId on the Open Data Portal
datasetID <- "pfjr-vzaw" 
url <- generateUrl(datasetID) 
happinessSurveyData <- RSocrata::read.socrata(url) 

```

```{r filterex3-check}
"test"

```

## Let's Explore this data some!

### Initial QA

Something I often do once the code will compile and run is assume that I've made every mistake possible, so I like to get an overview of the dataset. For a dataset as large as the happiness survey, it can be tough to just open up excel and take a peek, so I often do this in R.

```{r filterex4, exercise = TRUE}
dplyr::glimpse(happinessSurveyData)
```

I also often like to see if any of the rows are obviously junk, so I look to see if whatever unique identifier I'm using is NA.
```{r filterex5, exercise = TRUE}
nrow(happinessSurveyData %>% dplyr::filter(is.na(id)))
```

As we can see, the team did at least a bit of QA on this dataset before releasing it (thank goodness it would have been embarrassing if we found QA issues in the Happiness Survey this early in an R training).

Besides the joining that I showed you in the beginning, a large amount of what I do in R is dropping columns, especially those that may include sensitive information. A common issue that we have to tackle at SomerStat is a limited number of respondants who fit any of the questions we have talke about. We rarely want uniquely identifying information in any datset we release, and age can be uniquely identifying if it is a unique value from the rest of the dataset. We often group data to solve this problem: for example we have grouped age in the Happiness Survey. 

```{r filterex6, exercise = TRUE}
unique(happinessSurveyData$age)
```

After you have identified the issue with age, and bucketed out the ages you may think the problem is solved; however, you still need to be mindful of the size of these buckets. 

```{r filterex7, exercise = TRUE}
table(happinessSurveyData$age)
```

As we can see, there have only been 8 survey respondents ages 17 or younger that have submitted the Happiness Survey. Let's see what years these responses came in:

```{r filterex8, exercise = TRUE}
only17Younger <- happinessSurveyData %>% dplyr::filter(happinessSurveyData$age == "17 or younger") 
only17Younger$year
```

As we can see, there is only one respondent from 2019 that was 17 or younger and responded to the Happiness Survey. Is this a problem? Probably not, but this is exactly the kind of thing that we interrogate the data for to either find obvious problems or allow us to consciously make a decision about whether or not to further aggregate the data and make is less identifiable. This is especially true when we are dealing with more sensitive data, such as Police or fill in other sensitive data here. 

## Derek add an excercise here


## Okay you are now ready to start your own project, what does that look like?

This office uses github (kind of I swear) to track coding projects, allow for collaboration between many coders in the office, and add some sense of accountability in the code written. Sam has created a template repo for our projects as well, which includes the usual file format we use. This template is always a suggestion, and each project's needs are slightly different so please do not feel entrapped by it. Nevertheless I will explain how to use github for the limited uses we need in the office. If questions come up please do not hesitate to reach out to Sam or Derek (who will probably ask Sam either way but may be less scary since I'm also an intern). 

