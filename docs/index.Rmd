---
title: "SomerStat Intern Training"
output: 


learnr::tutorial:
progressive: true
allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(Lahman)
library(RSocrata)
library(stringr)
socrataUrl <-"https://data.somervillema.gov/resource" ## not sensitive
generateUrl <- function (datasetID) paste(
  socrataUrl, 
  paste(datasetID, 'json', sep="."), 
  sep="/"
)
chooseCRANmirror(ind = 1)
## all of this is actually pre-computed so the people can pick up anywhere in the exercise
datasetID <- "pfjr-vzaw"
url <- generateUrl(datasetID)   
happinessSurveyData <- RSocrata::read.socrata(url)
happinessSurveyData$happiness_5pt_num <- as.numeric(happinessSurveyData$happiness_5pt_num)
happinessSurveyData$police_department_satisfaction_5pt_num <- as.numeric(happinessSurveyData$police_department_satisfaction_5pt_num)
only17Younger <- happinessSurveyData %>% dplyr::filter(happinessSurveyData$age == "17 or younger") 
tutorial_options(exercise.eval = FALSE,
exercise.timelimit = 60,
# A checker function that compares the expected and actual results
exercise.checker = function(label, user_code, solution_code, check_code, envir_result, evaluate_result, ...) {
  tryCatch({
    if(user_code == solution_code){
      return(list(correct = TRUE, message = "That's exactly how we did it!"))
    }
    user_result <- eval(parse(text = user_code), envir = envir_result)
    correct_result <- eval(parse(text = solution_code), envir = envir_result)
    if (nrow(user_result) == nrow(correct_result) && all.equal(user_result, correct_result) == TRUE) {
      return(list(correct = TRUE, message = "Great job!"))
    } else {
      return(list(correct = FALSE, message = "Check your answer and try again."))
    }
  }, error = function(e) {
    return(list(correct = FALSE, message = "There was an error in the code execution."))
  })
}
)
knitr::opts_chunk$set(error = TRUE)
```

## Welcome to our R (pun intended) rundown!

To merge two quotes from Sam's Rolodex of sayings I've heard in the past week or so, "R is the second best tool to do 5 million things and there are 5 thousand ways of doing each of these things." This tutorial is not meant to be a resource that you have open as you start your first project, it is meant to allow you to use the three best tools of R progress (Google, Google, and Google) to effectively write code that looks similar to how the other members of SomerStat write code in R. In an attempt to have you see the value of this similar coding style, I will show you the code that I (Derek) wrote in my first major project and the functionally identical code that Sam would have told me to use if this training existed when I started. This is by no means law, and if you prefer something by all means tell us about it and there's a good chance we will agree and change how we do things! 

Derek's code to combine two datasets on a column (i.e. there is an id field shared between two files and I want to merge these columns):

```{r, eval = FALSE}
merged <- merge(x = merged, y = geolocationData, by = 'id', all.x = TRUE)
```

Sam's much more readable code:

```{r, eval = FALSE}
merged <- merged %>% dplyr::left_join(geolocationData, by = 'id')
```

For those natural coders among us, you may be about to google what I already have, and yes, under the hood left_join takes a very convoluted path to doing nearly exactly this base R line with lots of error checking. Dplyr abstracts a lot of common lines of code (like merging two datasets) away and makes the code much more readable, and is widely used throughout the office. Let's load in some data and try some other features of dplyr out! In case of getting stuck, all of the answers are in an R file that probably lives right next to the one you ran to open the browser on your localhost unless I miraculously fixed the web-hosting problem. We do not care / track if you ended up getting the little textbox to be green, I just thought it would make the tutorial a bit more fun and interactive.

The data we will be using comes from our annual Happiness Survey (insert description of this here), which this website reads from the [Open Data Portal](https://data.somervillema.gov/Health-Wellbeing/Somerville-Happiness-Survey-Responses-2011-2021/pfjr-vzaw/about_data)

There is a good chance that your time here will include analyzing data in R and/or reading/writing data to/from the Open Data Portal, so I hope this little tutorial is more helpful than harmful.

### Common mistakes

Me (Derek) thinking I can code in R

## Loading in the Happiness Survey Data from the Open Data Portal

This is roughly the workflow that I use to read data from the Open Data portal.

First I make sure to load in all the packages, I use pacman to do this nasty install vs load in business for me. 
```{r filterex1, exercise = TRUE}
if (!require(pacman)){
  install.packages("pacman") ## install pacman if you do not already have it
}
library(pacman) ## load pacman into this environment 

## this pacman::p_load abstracts away the above lines of code and this is all you have to do to get started with it!

pacman::p_load(RSocrata, tidyverse) ## this loads in RSocrata and the entire tidyverse, which includes dplyr and many of the packages we use!
```

```{r filterex2, exercise = TRUE}
## to access data on the open data portal, there are a few things you have to set up.
socrataUrl <-"https://data.somervillema.gov/resource" ## not sensitive
generateUrl <- function (datasetID) paste(
  socrataUrl, 
  paste(datasetID, 'json', sep="."), 
  sep="/"
)
```

```{r filterex3, exercise = TRUE}
## if the bottom of this exercise says 404 not found, you do not have the right datasetID
datasetID <- 'some-string' ## TODO
url <- generateUrl(datasetID)   
happinessSurveyData <- RSocrata::read.socrata(url)

```

```{r filterex3-hint-1}
# Check for the dataset id here, I typically find it in the url! <https://data.somervillema.gov/Health-Wellbeing/Somerville-Happiness-Survey-Responses/pfjr-vzaw/about_data>
# click Next Hint to see the solution!
```


```{r filterex3-solution}
## TODO, find the datasetId on the Open Data Portal
datasetID <- "pfjr-vzaw" 
url <- generateUrl(datasetID) 
happinessSurveyData <- RSocrata::read.socrata(url) 

```

```{r filterex3-check}
"test"
```

## Let's Explore this data some!

### Initial QA

Something I often do once the code will compile and run is assume that I've made every mistake possible, so I like to get an overview of the dataset. For a dataset as large as the happiness survey, it can be tough to just open up excel and take a peek, so I often do this in R.

```{r filterex4, exercise = TRUE}
dplyr::glimpse(happinessSurveyData)
```

I also often like to see if any of the rows are obviously junk, so I look to see if whatever unique identifier I'm using is NA.
```{r filterex5, exercise = TRUE}
nrow(happinessSurveyData %>% dplyr::filter(is.na(id)))
```

As we can see, the team did at least a bit of QA on this dataset before releasing it (thank goodness it would have been embarrassing if we found QA issues in the Happiness Survey this early in an R training).

Besides the joining that I showed you in the beginning, a large amount of what I do in R is dropping columns, especially those that may include sensitive information. A common issue that we have to tackle at SomerStat is a limited number of respondants who fit any of the questions we have talke about. We rarely want uniquely identifying information in any datset we release, and age can be uniquely identifying if it is a unique value from the rest of the dataset. We often group data to solve this problem: for example we have grouped age in the Happiness Survey. 

```{r filterex6, exercise = TRUE}
unique(happinessSurveyData$age)
```

After you have identified the issue with age, and bucketed out the ages you may think the problem is solved; however, you still need to be mindful of the size of these buckets. 

```{r filterex7, exercise = TRUE}
table(happinessSurveyData$age)
```

As we can see, there have only been 8 survey respondents ages 17 or younger that have submitted the Happiness Survey. Let's see what years these responses came in:

```{r filterex8, exercise = TRUE}
only17Younger <- happinessSurveyData %>% dplyr::filter(happinessSurveyData$age == "17 or younger") 
only17Younger$year
```

As we can see, there is only one respondent from 2019 that was 17 or younger and responded to the Happiness Survey. Is this a problem? Probably not (but we may actually consider this in a future meeting because it's interesting), but this is exactly the kind of thing that we interrogate the data for to either find obvious problems or allow us to consciously make a decision about whether or not to further aggregate the data and make it less identifiable. This is especially true when we are dealing with more sensitive data, such as police data or fill in other sensitive datasets that you will work on but currently only live in Anna's brain here. 

## Mean/median/stdev QA

Another very standard exercise is finding the mean/median/stdev of data. Let's try this with people's happiness here in Somerville. We convert all of the previous years scales to a 5 point scale in the happiness_5pt_num. It could be interesting to see how to mean/median/stdev over all of the years in the dataset. This exercise is a bit tricky and I'm sure there are perfectly acceptable answers that will not be accepted due to the fact that the checker is a function written by me (Derek) so do your best but don't stress too hard! Also [here](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) is some pretty good dplyr documentation that you may find helpful ;)

```{r filterex9, exercise = TRUE}
## this dataframe should have four columns: year, mean, median, and stdev with 7 rows.
QAdataframe <- 0 ## TODO
```

```{r filterex9-hint-1}
"I would look into the dplyr package documentation, especially for group_by and summarise."
```


```{r filterex9-solution}
QAdataframe <- happinessSurveyData %>%
    group_by(year) %>%
    summarise(mean = mean(happiness_5pt_num, na.rm = TRUE), median = median(happiness_5pt_num, na.rm = TRUE), stdev = sd(happiness_5pt_num, na.rm = TRUE))
```

```{r filterex9-check}
"Still not sure why there needs to be text here"
```

## One last dplyr excercise!

Back to the 17 or younger issue, you brought up your QA finding in this week's Happiness Survey meeting and the team was really impressed with this and we've decided to completely overreact to this finding (we can act like there was some really damning information in this or whatever you need to convince yourself this exercise is a real thing we would do, it's hard to find a data cleaning task on our cleaned dataset). We decided to remove each row (survey response) that was from someone who was 17 or younger and decided to drop the age column. You will probably want to refer to that dplyr cheatsheet once again!

```{r filterex10, exercise = TRUE}
## this dataframe should be just like the happinessSurveyData one, just without the rows that were the 17 or younger and without the age column. 
happinessDataCleaned <- 0 ## TODO
```

```{r filterex10-hint-1}
"I would look into the dplyr package documentation, especially for filter and select. You may also need to look into how you handle NA comparisions"
```


```{r filterex10-solution}
happinessDataCleaned <- happinessSurveyData %>% filter(!(age == "17 or younger" & !is.na(age))) %>% select(-age)
```

```{r filterex10-check}
"Still not sure why there needs to be text here"
```

## Okay you are now ready to start your own project, what does that look like?

This office uses github (kind of I swear) to track coding projects, allow for collaboration between many coders in the office, and add some sense of accountability in the code written. Sam has created a template repo for our projects as well, which includes the usual file format we use. This template is always a suggestion, and each project's needs are slightly different so please do not feel entrapped by it. Nevertheless I will explain how to use github for the limited uses we need in the office. If questions come up please do not hesitate to reach out to Sam or Derek (who will probably ask Sam either way but may be less scary since I'm also an intern). 

### Setting up the repo

Once again foreseeing the making of this training, Sam has already done the work that I will show you how to run to set up a repository for your project.

Firstly if you do not already have a github, make one and have Sam add you to the SomerStat team.

If you download the api_key and the new-repo-from-template-interns files from [here](https://somervillema.sharepoint.com/sites/SomerStat2/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2FSomerStat2%2FShared%20Documents%2FTools%2FGitHub%20Helper&viewid=1c921e9c%2Dd682%2D49c9%2D83c3%2Da5ee71285b85) you will be able to very easily create a new repo by running the batch file. A brand-new repo should appear, and you can rename it by going to settings in the top right of the nav pane. This repo includes the usual format that we use in a project, with a, rproj file that is a useful tool for Rstudio, a readme.md per usual github fashion, and finally folders for the data. figures, markdown figures, and scripts. Sam has done a great job explaining this template in the readme. so I will not steal his thunder here. 

### The first git clone (instructions [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent))
Now that you have a new repo, and probably have renamed it, we need to get it onto you machine so you can begin coding away in R. Use the terminal to cd into a new folder, and run the command ssh-keygen. Press enter to keep the defaults for all of the questions. There should now be a file called ~/.ssh/id_rsa in the folder you cd'd into, copy the content of that file that into github > Settings ? SSH and GPG Keys > New SSH key > key field and create a SSH key. (Please if you are doing this training get Derek for this part, my advice is entirely untested) Now that we have the proper authentication, we will git clone onto your computer by clicking on the <> Code green button on the github page, and then copying the url shown for HTTPS. You should mpw be able to clone the repo onto your machine by cd'ing into where you want to be in terminal, and running git clone url (where the url is the one you copied from the green code button).

### File management in git

If you use the file explorer to go to the directory you cloned into, you should see a new folder with the name of your repo. You should now do all of the R coding your heart desires until you get to a good break point. I usually check the state of the repo with "git status". You would then "git add ." to add all of the files. At this point I usually do "git status" again to see what all changes I have added, then "git commit -m"This is a commit message where you detail the changes you made in this commit"" stages these changes. Finally, git push will apply these changes to the repo on github. 

### Git branch

If you have a working version of something and want to make changes to be applied only in the future when we are sure our new version also works, we use "git branch". To pick something up and do this, I would cd to the directory, and then "git pull" to get any changes. Then "git branch new-branch-name" will create a branch for you to work on. All commits that you make will be push to this branch. 

This is a very simplified viewing of git, and if it is your first time using it please ask questions of the other people in the office sooner rather than later, as it is a very complicated piece of version control software. I would recommend reading at least one or two intro articles on it and maybe even talking to Sam to set up github desktop which can simplify things.
